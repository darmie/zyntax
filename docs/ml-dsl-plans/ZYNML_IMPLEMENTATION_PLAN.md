# ZynML Implementation Plan

## Executive Summary

This document outlines the implementation roadmap for ZynML, a unified domain-specific language for machine learning built on the Zyntax compiler infrastructure. ZynML provides a seamless experience for data scientists and ML engineers, combining Python-like ergonomics with systems-level performance.

## Current State Analysis

### Existing ZRTL Plugins (29 total)

| Plugin | Status | ML Relevance |
|--------|--------|--------------|
| `zrtl_simd` | âœ… Complete | **Critical** - SIMD-accelerated ML ops |
| `zrtl_tensor` | âœ… Complete | **Critical** - Core tensor data structure |
| `zrtl_vector` | âœ… Complete | **Critical** - Vector search & embeddings |
| `zrtl_audio` | âœ… Complete | **High** - Audio ML preprocessing |
| `zrtl_text` | âœ… Complete | **High** - Tokenization & text processing |
| `zrtl_model` | âœ… Complete | **High** - Model loading (SafeTensors) |
| `zrtl_math` | âœ… Complete | **High** - Basic math functions |
| `zrtl_image` | âœ… Complete | **High** - Image loading |
| `zrtl_json` | âœ… Complete | **Medium** - Config/metadata |
| `zrtl_fs` | âœ… Complete | **Medium** - File I/O |
| Others | âœ… Complete | Low - General utilities |

### What `zrtl_simd` Provides (40+ SIMD-accelerated functions)

```
ML Operations in zrtl_simd:
â”œâ”€â”€ Vector Operations (14 functions)
â”‚   â”œâ”€â”€ vec_dot_product_f32       - SIMD dot product
â”‚   â”œâ”€â”€ vec_cosine_similarity_f32 - SIMD cosine similarity
â”‚   â”œâ”€â”€ vec_euclidean_f32         - SIMD Euclidean distance
â”‚   â”œâ”€â”€ vec_euclidean_sq_f32      - SIMD squared Euclidean
â”‚   â”œâ”€â”€ vec_manhattan_f32         - SIMD Manhattan distance
â”‚   â”œâ”€â”€ vec_l2_normalize_f32      - SIMD L2 normalization
â”‚   â”œâ”€â”€ vec_sum_f32, vec_min_f32, vec_max_f32
â”‚   â”œâ”€â”€ vec_fill_f32, vec_scale_f32, vec_add_f32, vec_sub_f32, vec_mul_f32
â”‚   â”œâ”€â”€ vec_fma_f32, vec_fma_scalar_f32
â”‚   â””â”€â”€ vec_argmax_with_val_f32, vec_argmin_with_val_f32
â”œâ”€â”€ Matrix Operations
â”‚   â””â”€â”€ gemm_f32(a, b, c, m, n, k, alpha, beta) - General matrix multiply
â”œâ”€â”€ Activation Functions
â”‚   â”œâ”€â”€ relu_f32(data, len)
â”‚   â”œâ”€â”€ leaky_relu_f32(data, len, alpha)
â”‚   â”œâ”€â”€ sigmoid_f32(data, len)
â”‚   â”œâ”€â”€ tanh_f32(data, len)
â”‚   â””â”€â”€ softmax_f32(data, len)
â”œâ”€â”€ Normalization
â”‚   â”œâ”€â”€ layer_norm_f32(data, gamma, beta, len, eps)
â”‚   â””â”€â”€ batch_norm_f32(data, mean, var, gamma, beta, len, eps)
â”œâ”€â”€ Convolution & Pooling
â”‚   â”œâ”€â”€ conv2d_f32(input, kernel, output, params...)
â”‚   â”œâ”€â”€ max_pool2d_f32(input, output, params...)
â”‚   â””â”€â”€ avg_pool2d_f32(input, output, params...)
â””â”€â”€ Utilities
    â”œâ”€â”€ argmax_f32(data, len) -> index
    â”œâ”€â”€ clip_f32(data, len, min, max)
    â”œâ”€â”€ exp_f32(data, len), log_f32(data, len)
    â””â”€â”€ dropout_f32, cross_entropy_loss_f32
```

---

## Completed ML Plugins

### 1. `zrtl_tensor` - Core Tensor Operations âœ…

**Status: COMPLETE** (8 tests passing)

The foundational data structure for all ML operations, now SIMD-optimized.

**Implemented Features:**
- Multi-dimensional tensor with arbitrary shapes (up to 8 dims)
- Multiple data types (F32, F64, I8, I16, I32, I64, U8, U16, U32, U64, Bool, F16, BF16)
- Reference-counted memory management with views
- Copy-on-write semantics

**SIMD-Optimized Operations:**
| Function | SIMD Function Used |
|----------|-------------------|
| `tensor_ones` | `vec_fill_f32` |
| `tensor_full_f32` | `vec_fill_f32` |
| `tensor_sum_f32` | `vec_sum_f32` |
| `tensor_max_f32` | `vec_max_f32` |
| `tensor_min_f32` | `vec_min_f32` |
| `tensor_argmax_f32` | `vec_argmax_with_val_f32` |

**API:**
```rust
// Creation
tensor_new, tensor_zeros, tensor_ones, tensor_full_f32
tensor_arange_f32, tensor_linspace_f32, tensor_rand_f32, tensor_randn_f32

// Memory
tensor_free, tensor_clone, tensor_view, tensor_contiguous

// Info
tensor_ndim, tensor_shape, tensor_stride, tensor_numel, tensor_dtype
tensor_data, tensor_is_contiguous

// Element Access
tensor_get_f32, tensor_set_f32, tensor_get_at_f32, tensor_set_at_f32

// Shape Operations
tensor_reshape, tensor_transpose, tensor_squeeze, tensor_unsqueeze, tensor_slice

// Reductions
tensor_sum_f32, tensor_mean_f32, tensor_max_f32, tensor_min_f32, tensor_argmax_f32

// Type Conversion
tensor_to_dtype
```

---

### 2. `zrtl_audio` - Audio Processing âœ…

**Status: COMPLETE** (4 tests passing)

Essential for speech/audio ML models, now SIMD-optimized.

**SIMD-Optimized Operations:**
| Function | SIMD Functions Used |
|----------|-------------------|
| `audio_normalize` | `vec_max_f32` + `vec_scale_f32` |
| `audio_mel_spectrogram` | `vec_dot_product_f32` (filterbank) |

**Dependencies:**
- `symphonia` - Audio decoding (MP3, WAV, FLAC, OGG)
- `rubato` - High-quality resampling
- `rustfft` - FFT for spectrograms

**API:**
```rust
// Loading
audio_load, audio_free

// Info
audio_sample_rate, audio_channels, audio_duration_samples
audio_duration_seconds, audio_samples

// Processing
audio_to_mono, audio_resample, audio_normalize, audio_trim_silence

// Feature Extraction
audio_stft, audio_stft_free, audio_mel_spectrogram
```

---

### 3. `zrtl_text` - Text & Tokenization âœ…

**Status: COMPLETE** (7 tests passing)

Essential for NLP/LLM applications.

**Implemented Features:**
- BPE tokenizer implementation
- Text preprocessing (lowercase, strip accents, normalize whitespace)
- Unicode normalization (NFC, NFD, NFKC, NFKD)
- Text chunking for RAG pipelines

**API:**
```rust
// BPE Tokenizer
bpe_create, bpe_free, bpe_add_merge, bpe_train_from_text
bpe_encode, bpe_decode, bpe_vocab_size

// Text Processing
text_normalize_unicode, text_lowercase, text_strip_accents
text_normalize_whitespace, text_chunk_by_sentences, text_chunk_by_tokens
```

---

### 4. `zrtl_model` - Model Loading & Serialization âœ…

**Status: COMPLETE** (3 tests passing)

Load and save ML model weights in SafeTensors format.

**Implemented Features:**
- SafeTensors format loading
- Memory-mapped file access for large models
- F16/BF16 to F32 conversion
- Tensor inspection and extraction

**Dependencies:**
- `safetensors` - SafeTensors format (HuggingFace standard)
- `memmap2` - Memory-mapped file access

**API:**
```rust
// Loading
model_load, model_free

// Info
model_num_tensors, model_tensor_names, model_tensor_names_free

// Tensor Access
model_get_tensor_info, model_get_tensor_f32, model_tensor_free
```

---

### 5. `zrtl_vector` - Vector Search & Embeddings âœ…

**Status: COMPLETE** (6 tests passing, including HNSW)

For RAG, semantic search, and embedding operations. Fully SIMD-optimized.

**SIMD-Optimized Operations:**
| Function | SIMD Function Used |
|----------|-------------------|
| `vector_cosine_similarity` | `vec_cosine_similarity_f32` |
| `vector_dot_product` | `vec_dot_product_f32` |
| `vector_euclidean_distance` | `vec_euclidean_f32` |
| `vector_euclidean_distance_sq` | `vec_euclidean_sq_f32` |
| `vector_manhattan_distance` | `vec_manhattan_f32` |
| `vector_l2_normalize` | `vec_l2_normalize_f32` |

**Implemented Features:**
- Similarity functions (cosine, dot product, Euclidean, Manhattan)
- Batch similarity computation
- Top-K search with various metrics
- Flat index (exact brute-force search)
- HNSW index (approximate nearest neighbor)
- L2 normalization and pooling

**API:**
```rust
// Similarity Functions
vector_cosine_similarity, vector_dot_product
vector_euclidean_distance, vector_euclidean_distance_sq
vector_manhattan_distance, vector_cosine_similarity_batch

// Normalization
vector_l2_normalize, vector_l2_normalize_batch

// Pooling
vector_mean_pooling, vector_cls_pooling

// Top-K Search
vector_topk_cosine

// Flat Index (exact search)
flat_create, flat_free, flat_add, flat_len, flat_search_cosine

// HNSW Index (approximate search)
hnsw_create, hnsw_free, hnsw_add, hnsw_len, hnsw_search
```

---

## Implementation Progress

### Phase 1: Core Foundation âœ… COMPLETE

| Task | Status | Tests |
|------|--------|-------|
| zrtl_tensor handle and memory layout | âœ… | 8/8 |
| zrtl_tensor creation functions | âœ… | âœ“ |
| zrtl_tensor shape operations | âœ… | âœ“ |
| zrtl_tensor SIMD optimization | âœ… | âœ“ |
| zrtl_audio loading (symphonia) | âœ… | 4/4 |
| zrtl_audio resampling (rubato) | âœ… | âœ“ |
| zrtl_audio STFT/mel spectrogram | âœ… | âœ“ |
| zrtl_audio SIMD optimization | âœ… | âœ“ |

### Phase 2: Text & Models âœ… COMPLETE

| Task | Status | Tests |
|------|--------|-------|
| zrtl_text BPE tokenizer | âœ… | 7/7 |
| zrtl_text preprocessing | âœ… | âœ“ |
| zrtl_text chunking for RAG | âœ… | âœ“ |
| zrtl_model SafeTensors loading | âœ… | 3/3 |
| zrtl_model memory mapping | âœ… | âœ“ |
| zrtl_model dtype conversion | âœ… | âœ“ |

### Phase 3: Vector Search âœ… COMPLETE

| Task | Status | Tests |
|------|--------|-------|
| zrtl_vector similarity functions | âœ… | 6/6 |
| zrtl_vector SIMD optimization | âœ… | âœ“ |
| zrtl_vector Flat index | âœ… | âœ“ |
| zrtl_vector HNSW index | âœ… | âœ“ |
| zrtl_vector normalization | âœ… | âœ“ |

### Phase 4: ZynML Grammar & Compiler ðŸ”² NOT STARTED

| Task | Status |
|------|--------|
| Define `ml.zyn` grammar file | ðŸ”² |
| Tensor type annotations | ðŸ”² |
| Pipe operator semantics | ðŸ”² |
| Semantic analysis | ðŸ”² |
| Code generation | ðŸ”² |

---

## Test Summary

| Plugin | Tests | Status |
|--------|-------|--------|
| `zrtl_simd` | 26 | âœ… All passing |
| `zrtl_tensor` | 8 | âœ… All passing |
| `zrtl_audio` | 4 | âœ… All passing |
| `zrtl_text` | 7 | âœ… All passing |
| `zrtl_model` | 3 | âœ… All passing |
| `zrtl_vector` | 6 | âœ… All passing |
| **Total** | **54** | âœ… **All passing** |

---

## SIMD Optimization Summary

### New SIMD Functions Added (14 functions)

| Function | Description | Used By |
|----------|-------------|---------|
| `vec_fill_f32` | Fill array with constant | tensor |
| `vec_sub_f32` | Element-wise subtraction | vector |
| `vec_euclidean_f32` | Euclidean distance | vector |
| `vec_euclidean_sq_f32` | Squared Euclidean | vector |
| `vec_manhattan_f32` | Manhattan distance | vector |
| `vec_abs_sum_f32` | Sum of absolute values | - |
| `vec_fma_f32` | Fused multiply-add | - |
| `vec_fma_scalar_f32` | Scalar FMA | - |
| `vec_cosine_similarity_f32` | Cosine similarity | vector |
| `vec_l2_normalize_f32` | L2 normalization | vector |
| `vec_argmax_with_val_f32` | Argmax with value | tensor |
| `vec_argmin_with_val_f32` | Argmin with value | - |

---

## ZynML Grammar Preview

```zynml
// ml.zyn grammar (ZynPEG format)

tensor_type = "Tensor" "<" dtype "," shape ">" ;
dtype = "f32" | "f16" | "bf16" | "i8" | "i32" ;
shape = "[" (dim ("," dim)*)? "]" ;
dim = NUMBER | "?" | IDENT ;

// Pipe operator for data flow
pipe_expr = expr ("|>" IDENT ("(" args ")")?)* ;

// Compute block for GPU kernels
compute_block = "compute" "(" device ")" "{" stmts "}" ;
device = "cuda" (":" NUMBER)? | "cpu" ;

// Model definition
model_def = "model" IDENT "{" layer_defs "}" ;
layer_def = IDENT ":" layer_type "(" params ")" ;
```

**Example ZynML Program:**
```zynml
import audio, tensor, model

// Load and preprocess audio
let mel = audio.load("speech.wav")
    |> resample(16000)
    |> mel_spectrogram(n_mels=80)
    |> normalize()

// Load Whisper model
let whisper = model.load("openai/whisper-tiny")

// Run inference
compute(cuda) {
    let features = whisper.encoder(mel)
    let tokens = whisper.decoder.generate(features, max_len=256)
}

// Decode output
let text = tokenizer.decode(tokens)
print(text)
```

---

## Next Steps

1. **ZynML Grammar**: Define the ZynML grammar in ZynPEG format
2. **Type System**: Implement tensor type inference and shape propagation
3. **Code Generation**: Lower ZynML to ZRTL plugin calls
4. **Examples**: Build end-to-end examples (MNIST, Whisper, RAG)
5. **GPU Support**: Add CUDA backend for compute blocks

---

## Success Metrics

| Metric | Target | Current |
|--------|--------|---------|
| ML Plugin count | 5 | âœ… 5 complete |
| Test coverage | >80% | âœ… 54 tests |
| SIMD functions | 30+ | âœ… 40+ functions |
| Vector search | Working | âœ… Flat + HNSW |
| Audio processing | Working | âœ… STFT + Mel |
| Model loading | Working | âœ… SafeTensors |

---

*Document created: December 2024*
*Last updated: December 2024*
*Status: Phase 1-3 Complete, Phase 4 Pending*
